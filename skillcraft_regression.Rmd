---
title: "SkillCraft Multivariant Regression"
author: "Arrington Walters"
date: "11/4/2020"
output:
  pdf_document: default
  html_document: default
  word_document: default
bibliography: ./inline/bibliography.bib
editor_options:
  chunk_output_type: inline
---

```{bash git, eval=FALSE, include=FALSE}
cd ~/STAT757/skillcraft
git add --all

git commit -m "day before turn in"
git push

#git log
git status
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r loading_packages,include=FALSE}
library(tidyverse)
library(dbplyr) #piping
library(ggplot2) #plotting
library(gridExtra)# easy plot grids
library(Hmisc) # for correlation matrix
library(corrplot) # For correlation matrix graphic

#library(SC2API) #starcraft 2 API
library(broom) #tidy lm summaries
library(knitr) #pretty tables
library(reshape2) #melt function

```




```{r settoken, include=FALSE}
#set_token("33be678eb46d4f51ac36f72218abcdd2", #"Sb3QWR8A9mN9s0XgAt5w4j0FttY84pkg")
```

```{r Assignement, include=FALSE}
# Assignment Midterm 2

# https://cgrudz.github.io/teaching/stat_757_2020_fall/assets/midterm_2.html

# Final

# https://cgrudz.github.io/teaching/stat_757_2020_fall/assets/final_project.html
```
# Introduction
 
Videogames are one of my favorite past times. As a player who participates in ranked play, I've always kept my eye on the forefront of global competitions. One of the most notable games to establish an international competitive scene backed by paid professionals was the real-time strategy game (RTS) Starcraft II (SC2). In 2013 the top 10 starcraft players made nearly four-million dollars from their combined winnings @Winnings2019. Watching top-caliber players reflexes and control is astonishing to even seasoned videogame enthusiasts. At the 2019 StarCraft II World Championship Finale, many others and I packed into the arena to see what these professions could do firsthand.

!['@SC2IIWCSGlobal'](./inline/WCS.png)

The eye-watering speeds they perform at is universally referenced in gaming terminology as actions per minute (APMS). Professionals take actions at such fast speeds (high APMS); it becomes challenging to follow their overall strategy. Past pondering their sheer speed, I found it difficult to distinctly define what made these players high skilled.

To learn more about what defines talent in SC2 this analysis, we will explore in-game metrics to explain rank in competitive mode. The dataset used was provided by ['@UCI'](https://archive.ics.uci.edu/ml/datasets/SkillCraft1+Master+Table+Dataset).

## Goal

To model the response LeagueIndex a sample of player data from a 2013 ranked season of Starcraft will be explored. The predictors provided summarize in-game performance metrics for a season by player (GameID). The modeling process will consider all the predictor variables and then trim down until only significant predictors remain.  Variables will be vetted for multicollinearity, and finally, the model will be explored to see if the BLUE assumptions hold.

The goal of this analysis will be to test the explanatory power of APMs and other predictors that are less commonly discussed.

## Limitations of the Model

The multivariate regression model used for the midterm two portions of this study explores the linear estimation of mean response of LeagueIndex estimated by predictors in the design matrix $X$. 

The assumptions of this model's explanatory power depend on the residual error being gaussian. Considering LeagueIndex is an ordinal variable, it is doubtful, if not impossible, for the residuals to be statistically normal.

A more suitable form of a model for this regression would be based on a Polytonomous Logistic Regression for Ordinal Response (Proportional Odds Model) @OrdinalLog. These methods will be revisited for the final portion of this analysis.

# The Data Exploring

```{r loaddata , include=FALSE}
sc<-read_csv("~/STAT757/skillcraft/SkillCraft1_Dataset.csv")
```

This dataset is a sample of averaged in-game metrics of Starcraft II players who participate in 2013 ranked play. The variables are as follows:
```{r,echo=FALSE}
colnames(sc)
```
The appendix covers each in-depth, but the following are highlighted as a preface.

**LeagueIndex** The levels of LeagueIndex range 1-8 corresponding to player ranks Bronze, Silver, Gold, Platinum, Diamond, Master, Grand Master. Visible to the player in-game, each medal bronze through master is subdivided into divisions 1-5. Each division is once again but instead by divisions but is instead unbounded in terms of rank points @Leagues2019. The rating system similar to an Elo rating system standard in chess. Elo's designs have an extreme value distribution, also known as a Gumbel distribution @ELO. Although a Gumbel distribution would be problematic as a nonnormal response, it would provide some much-needed continuity by transforming players **LeagueIndex** into a more continuous experimental variable. Unfortunately, these subdivisions are either unavailable or would require far too much cleaning for the scope of this analysis. 

The limitation of predicting this ordinal response will be revisited more precisely, along with the exploration, modeling, and predictions.

The following are the icon's earned for players who achieve related rank by the end of a given season. The legends for the following plots are styled to match.

!['@SC2IIWCSGlobal'](./inline/StarCraft-II-Leagues.png) 

```{r colorvector, echo=FALSE}
cbPalette <- c("#CC6600", "#999999", "#FFCC00", "#CCCFFF", "#CCFFFF","#0072B2", "#FF6600")
```

**Actions Per Minute (APMs)** - APMs apply to various games but are the standard metric for analyzing proficiency of players at RTS games; its theorized skills like this provide a great advantage to players @APM. Action quickness alone does not capture the strategy or macro/micro-skills, so these additional predictors may add some unique color in hopes of further explaining what makes players skilled.

**Perception-Action Cycles (PACs)** - are the circular flow of information between an organism and its environment where a sensor-guided sequence of behaviors is iteratively followed towards a goal @Perception. In this data-set, PACs are aggregate of screen movements where PAC is a screen fixation of containing at least one action @UCI.

# Cleanliness

```{r ,include=FALSE}
#set type
sc$HoursPerWeek<-as.numeric(sc$HoursPerWeek)
sc$TotalHours<-as.numeric(sc$TotalHours)

count_missing_age<-count(sc%>%
  filter(is.na(Age))%>%arrange(LeagueIndex))
count_professional<-count(sc%>%filter(LeagueIndex==8))
count_grandmaster<-count(sc%>%filter(LeagueIndex==8))
print(paste('There are ',count_missing_age,' missing values in the age column. There are ',count_professional,' professional #players.'))
```

The missing values are related exclusively to players with LeagueIndex equivalent to Professional Players (8). The `r count_professional` players with LeagueIndex==8 the age data is NA and the HoursPerWeek are 0. LeagueIndexes 1-7 are obtainable playing matches in the base game and ranking up by winning. To be a professional, you would have to be part of a team that has no direct role in the broader matchmaking system. This study aims to understand how players go from being average to good, less so elite to best. The 55 values associated with professionals will be dropped to resolve both issues.

Another issue with **LeagueIndex** is that **LeagueIndex** 1-6 may contain many players, while **LeagueIndex** 7-Grandmaster may only include some set range of players targeted at 1000 total per region @Leagues2019. Dropping **LeagueIndex**=7 would be a step towards normality. Considering this multivariate linear model is already hampered by its selected application on an ELO system, **LeagueIndex**=7 will be kept to preserve a potential insight into Starcraft II players' larger population. 

In addition to the missing values we have a clear error with the **TotalHours** of one player. $GameID = 5140$ has 1,000,000 **TotalHours** that equates to 114 years of game time.

If we assumed one extra zero had been added at the end of the player's **TotalHours**, it equates to 14 years of playing time on a game that is only ten years old as of 2020. Removing two zeros equates to 1.4 years of playing time and three zeros in `r .14*365` days of played time, both that seem just as realistic. There is not a clear path to extrapolate this player's true **TotalHours**, so their data will be dropped from the analysis. This was initially detected during modeling but brought earlier into that analysis.

```{r,echo=FALSE}
sc<-filter(sc,sc$TotalHours<1000000)
```
Finally, performing a necessary inspection on **HoursPerWeek**, a max value of 168 was discovered. Considering there are 168 hours in a week, it's not plausible for an individual player to do this. There could be multiple players using this account, making this possible. Another prospect is that this player is an AI like google's DeepMind @AlphaStar. Either way, this observation will be kept because what is the realistic cutoff for hours per week is not apparent, and after removing this observation, the next max value is 140, which seems almost as unrealistic.

It's worth noting that dropping any amount of high hour outliers still far from combats all the potential abnormalities encountered through the use of **HoursPerWeek** and **TotalHours**. Multiple players could be using any of the accounts, even if either time played variable is not relatively large. Potentially exacerbating the left-extrema is that nothing prevents one player from smurfing multiple times. Smurfing is when a player makes an additional accounts @Smurf. A common reason for doing this is to dominate the competition until their Elo rating adapts to their actual skill level.

```{r finalclean, include=FALSE ,echo=FALSE}
sc<-sc%>%
  drop_na()%>%
  filter(HoursPerWeek!=0)
sc_describe<-describe(sc)
```

## Converting Units

Some of the time, averaged metrics are per SC2 timestamp while others are per millisecond. All-time metrics in timestamps or milliseconds will be converted to seconds to help with interpretability. There are roughly 88.5 timestamps per second, so each metric in timestamps will be multiplied by 88.5 @UCI. Some of the time-averaged metrics are per millisecond. This transformation is linear and will not affect our model's assumptions.
```{r changeunits, include=FALSE}
sc<-sc%>%
  mutate(NumberOfPACs=NumberOfPACs*88.5,
         MinimapAttacks=MinimapAttacks*88.5,
         MinimapRightClicks=MinimapRightClicks*88.5,
         SelectByHotkeys=SelectByHotkeys*88.5,
         AssignToHotkeys=AssignToHotkeys*88.5,
         UniqueHotkeys=UniqueHotkeys*88.5,
         WorkersMade=WorkersMade*88.5,
         UniqueUnitsMade=UniqueUnitsMade*88.5,
         ComplexUnitsMade=ComplexUnitsMade*88.5,
         ComplexAbilitiesUsed=ComplexAbilitiesUsed*88.5,
         GapBetweenPACs=GapBetweenPACs*1000,
         ActionLatency=ActionLatency*1000)
```

## Summary Statistics and Plots

### Gaussianity of the Response

When using the Shapiro-Wilk W test on response **LeagueIndex**, the null hypothesis that the sample comes from normally distribution can be rejected. Besides the obvious issues with performing a W test with an ordinal response with a potentially underlying Gumbel distribution, the response has a negative skew with a mean of `r round(mean(sc$LeagueIndex),2)`. Furthermore, there is no reason that **LeagueIndexes** are uniforming spaced in terms of overall rank or skill.

```{r, echo=FALSE}
LeagueIndex_Normal<-shapiro.test(sc$LeagueIndex)
  
ggplot(sc)+
  geom_histogram(aes(x=LeagueIndex,y=(..count..)/sum(..count..),fill=LeagueIndex),
      position = "identity", binwidth = 1,fill=cbPalette) +
  ylab("Relative Frequency")+
  ggtitle('LeagueIndex Distribution',subtitle = paste(LeagueIndex_Normal[3],
      " P-Value: ",LeagueIndex_Normal[2]))+xlab("LeagueIndex 1-Bronze to 7-Grandmaster")+theme_classic()

```

### Correlation Plot

Using the correlation matrix provided below, we can see that LeagueIndex has a relatively strong correlation with **APM, SelectByHotkeys, AssignToHotkeys, NumberofPACs, GapBetweenPACs, and Action Latency**. Some of these predictors may be the best choices for the model. However, it's worth noting that many of the predictor values also have reasonably strong correlations within themselves, which may cause multiple colinearities in a model. This is not too surprising because many of these metrics capture the rate of actions in slightly different forms. For example, **APM** and **NumberOfPacs** likely have a strict mathematical relationship where approximately. $$NumberOfPACs \approx APM*MatchDurationMinutes$$ The slight differences between these metrics could have some deep explanatory power, but that level of exploration is beyond the scope of this analysis. Focusing exclusively on **APM** fits into an Occam's razor approach by minimizing $span(X)$.

The following columns will be dropped as they may confound with **APMs, ActionLatency, GapBetweenPACs** ^[An additional issue with this predictor is that it does not seem to line up with the time units in the description. Before and after the unit transformation **GapBetweenPACs** results in a mean is `r round(mean(sc$GapBetweenPACs))/60^2` hours.)]**, NumberofPACS, SelectbyHotkey, and ActionsInPAC.**

```{r echo=FALSE}
sc_cor<-cor(select_if(sc,is.numeric),use = "complete.obs")
sc_cor_plot<-corrplot(sc_cor,
    tl.cex=.75,
    tl.col='black',
    type="lower",)
```

```{r dropconfounding, echo=FALSE}
sc_n<-sc

sc<-sc%>%dplyr::select(!c(GameID,ActionLatency,GapBetweenPACs,NumberOfPACs,SelectByHotkeys,ActionsInPAC))
```
 
### Visual Trend Analysis

```{r,include=FALSE}
cor_hoursperweek<-paste(
  round(cor(sc%>%filter(between(LeagueIndex,1,4))%>%dplyr::select(LeagueIndex),
    sc%>%filter(between(LeagueIndex,1,4))%>%dplyr::select(HoursPerWeek))[1],
    2),
  "vs",
  round(cor(sc%>%filter(between(LeagueIndex,4,7))%>%dplyr::select(LeagueIndex),
    sc%>%filter(between(LeagueIndex,4,7))%>%dplyr::select(HoursPerWeek))[1],
    2)
)
```

Visually determining trends between the predictors and responding with an ordinal response is best done with alternatives to scatter plots. Violin plots will be used to gauge the linearity concerning the response and distribution with the variable at the varying levels @ViolinPlots. The appendix covers more details concerning why Violin plots were chosen.

**MinimapAttacks, HoursPerweek, TotalHours, MinimapRightClicks, ComplexUnitsMade, ComplexAbilitiesUsed** all have very long right tails. In search of gaussian predictors, these predictors could be transformed for linearity. Although a transform would have affected the explanation's simplicity.^[A log transformation would be preferable, but enough observations by GameID that contain at least 0 in the related predictor entry would have to drop observations containing $-\infty$. If a transformation is pursued in the second half of this analysis, it will likely be a square root transformation]

#### No Relationship

**Age** the mean age of `r round(mean(sc$Age))` does not vary much across **LeagueIndex** such that there is no stark linear relationship. However, the variance at the highest level seems to be much narrower than that at the lower levels. 

#### Bimodal

**HourPerWeek** has visibly little or no relation to **LeagueIndex** 1-4, where **LeagueIndex** 4-7 seems to have a visible linear trend resulting in `r cor_hoursperweek`. If **HoursPerWeek** survives the model trimming, it's bimodality may cause issues with the model's assumption $COV(Y)=\sigma^2I$.**Workersmade, ComplexUnitsMades, and ComplexAbilityUsed** both have similar differences between **LeagueIndex** 1-4 and 4-7 with the portions that have no relation and a linear relation swapped in comparison to **HoursPerWeek**. 

#### Linear

**TotalHours, MinimapRightClicks, TotalMapExplored, and UniqueUnitsMade** have a positive linear trend with the response. **APM** has a strong linear relationship with the response. 

#### Root

**AssignToHotkeys, UniqueHotkeys, and MinimapAttacks** have a unique square root relationship with the response. This also may cause issues with the Gaussianity of the model's residuals.

```{r transform, echo=FALSE}
#sc[,c("MinimapAttacks", "HoursPerWeek","TotalHours","MinimapRightClicks"
#      ,"ComplexUnitsMade","ComplexAbilitiesUsed")]<-sqrt(sc[,c("MinimapAttacks", "HoursPerWeek","TotalHours","MinimapRightClicks"
#      ,"ComplexUnitsMade","ComplexAbilitiesUsed")])
```

```{r,echo=FALSE,error=FALSE, compact=TRUE}
VioLeagueIndex<-function(predictor){
  ggplot(sc, aes(x=factor(LeagueIndex), y=unlist(sc%>%select(all_of(predictor))), fill=factor(LeagueIndex))) + 
    geom_violin(trim=FALSE, color="black")+scale_fill_manual(values=cbPalette)+
    stat_summary(fun.data=mean_sdl,geom="pointrange", color="black")+ coord_flip()+
    ggtitle(paste("LeagueIndex by",predictor))+
    xlab("LeagueIndex")+ylab(predictor) + guides(fill= FALSE)+theme_classic()
    }

plots<-lapply(colnames(sc)[2:length(colnames(sc))],VioLeagueIndex)

do.call("grid.arrange", c(plots[1:4], ncol=2))
do.call("grid.arrange", c(plots[5:8], ncol=2))
do.call("grid.arrange", c(plots[9:13], ncol=2))
```

# Model Specifications

## Multivariant Regression Model Manual Model Iterations ($\Omega$ to $\omega$)

```{r modeltrimming,echo=FALSE}
sc_lm<-lm(LeagueIndex~.,sc)
sc_lm_1<-update(sc_lm,.~.-UniqueUnitsMade ,sc)
sc_lm_2<-update(sc_lm,.~.-Age-UniqueUnitsMade)
sc_lm_3<-update(sc_lm,.~.-Age-UniqueUnitsMade-ComplexUnitsMade)
sc_lm_4<-update(sc_lm,.~.-Age-UniqueUnitsMade-ComplexUnitsMade-MinimapRightClicks)
sc_lm_final<-update(sc_lm,.~.-Age-TotalMapExplored-UniqueUnitsMade-MinimapRightClicks-ComplexUnitsMade)
```

A model with all predictors will be made. Subsequently, predictors will be dropped one by one until only predictors with significance of at least $\alpha=5\%$ remain starting with $lm_\Omega$ and ending with $lm_\omega$. The results are as follows:

The initial model has many insignificant predictors. The $lm_\Omega$ summary:
```{r, echo=FALSE,compact=FALSE}
kable(tidy(sc_lm,conf.level = .05),caption = "Summary of Starting Manual Stepwise Backward Model Selection $lm_\\Omega$")
```

**Age, UniqueUnitsMade, ComplexUnitsMade, MinimapRightClicks, and TotalMapExplored** were removed in that order across the model's five iterations. All remaining predictors were significant to the predetermined $\alpha$. The final iteration provided:

```{r, echo=FALSE}
kable(tidy(sc_lm_final,conf.level = .05),caption = "Summary of Final Manual Stepwise Backward Model Selection$lm_\\omega$")
```

### Assessing Fit and Overall Significance

A test will be performed to see if the predictors provide statistically significant better model than the null model $Y=\overset{\_}{Y}+\epsilon$ where $Y=LeagueIndex$ . The hypothesis are as follows:

$$H_o:\beta=0$$
$$H_a:LeagueIndex=X\beta+\epsilon$$

Without much surprise using `r length(colnames(sc))-1` predictor variables results of a very small p-value of ~`r glance(sc_lm)$p.value `. Over the iteration, this does not change notably across the other models as the last model also results in a p-value ~`r glance(sc_lm_4)$p.value`. Thus all $\Delta p=p_\Omega-p_\omega$ iterations of the model, we can reject the null hypothesis suggesting that we should further investigate the explanatory power of our alternative hypothesis. 

## Testing for Significance Between Models

If both models had normal residuals, an F-test could be used on $lm_\Omega$ and $lm_\omega$ to determine if the models have significantly different residuals. $RSS_\Omega$ and $RSS_\omega$ both have Shapiro-Wilk's test statistics that reject the null at $\alpha=5\%$ shown in a later section that examines the normality of each models' residuals. Without gaussian residuals conducting an ANOVA will be the only practice exercise for the final where the hypothesis is provided by:

 $$ H_o:RSS_\omega = RSS_\Omega$$
 $$ H_a:RSS_\omega \neq RSS_\Omega$$
 
Performing an ANOVA test below, we find an insignificant difference in the models at $\alpha=5\%$ such that we cannot reject the $H_o$. The implications not rejecting $H_o$ is that regardless of trimming the predictor space by $\Delta p$ predictors, $lm_\omega$ is expected to produce comparable residuals with a 5% chance this is a result of the sampling. Additionally, their $adjR^2$ is barely different. Where  $adjR^2_\Omega=$ `r round(summary(sc_lm)[["adj.r.squared"]],4)` and $adjR^2_\omega=$ `r round(summary(sc_lm_final)[["adj.r.squared"]],4)`. Further analysis will be conducted to see if the predictive and explanatory power of the models differs past the magnitude of their residuals.

```{r,echo=FALSE, compact=TRUE}
kable(anova(sc_lm,sc_lm_final),digits=2)
```
### Confidence Intervals

The confidence interval Table^[Delta values are normalized by dividing by the mean of the model by related predictors confidence interval] show only a few subtleties between models coefficients confidence intervals. 

In the starting model **Age, MinimapRightClicks, UniqueUnitsMade, ComplexUnitsMade, and ComplexAbilitiesUsed** are all not significant based on their p-value, and their confidence intervals straddle 0. Interestingly enough, **ComplexAbilitiesUsed** was initially above the alpha value for significance but made it to the final model. This could be because of the removal of a confounding variable. The magnitude of this shift is reflected in its delta value and delta_width.

For comparison, two summary statistics were added as follows:

*Where:*
$$delta=\frac{(UL_\omega+LL_\omega)-(UL_\Omega+LL_\Omega)}{(UL_\omega+LL_\omega)}=\frac{MeanCI_\omega-MeanCI_\Omega}{MeanCI_\omega}$$
$$delta_{width}=\frac{(UL_\omega-LL_\omega)-(UL_\Omega-LL_\Omega)}{(UL_\omega-LL_\omega)}$$
```{r,echo=FALSE}
#t<-data.frame(confint(sc_lm),confint(sc_lm_final))
t1 <- data.frame(confint(sc_lm))
t2 <- data.frame(confint(sc_lm_final))
t3<-merge(t1,t2,by="row.names",all=TRUE)
t3<-t3%>%rename(LL_s=X2.5...x,UL_s=X97.5...x,LL_f=X2.5...y,UL_f=X97.5...y)
t3<-t3%>%mutate(mean_s=(UL_s+LL_s)/2,
                mean_f=(UL_f+LL_f)/2,
                delta=(mean_f-mean_s)/mean_f,
                delta_width=((UL_f-LL_f)-(UL_s-LL_s))/(UL_f-LL_f))%>%
         mutate_if(is.numeric, ~round(., 3))
kable(t3,format = "markdown", digits = c(4,4,4,4,4,4,4,2,2), caption="Confidence Intervals Statistics at $\\alpha$ 0.05" )
```

### Confounding

For reference in the following analysis of confounding variables, the new correlation matrix provides a zoomed view of the target of discussion.

```{r omegecor,echo=FALSE}
sc_omega<-sc%>%select(HoursPerWeek,TotalHours,HoursPerWeek,APM,AssignToHotkeys,WorkersMade,ComplexUnitsMade,ComplexAbilitiesUsed)

sc_omega_cor<-cor(select_if(sc_omega,is.numeric),use = "complete.obs")
sc_omega_cor_plot<-corrplot(sc_omega_cor,
    tl.cex=.75,
    tl.col='black',
    method="number",
    type="lower")
```

#### Complex Units/Abilities

As mentioned when exploring the confidence intervals, the notable change in **ComplexAbilitiesUsed**'s confidence interval between the $lm_\Omega$ to $lm_\omega$ is likely a result of the removal of the variable **ComplexUnitsMade**. Upon reintroducing **ComplexUnitsMade** into the final model, we find the following p-values for both predictors to be insignificant. The difference in P-value is a warning that we may not distinguish the effects or vary them independently. This aligns with what is expected based on the mechanics of the game. Players must make complex units before they can use their complex abilities. 

The way forward with the model is to continue leaving out **ComplexUnitsMade** because only making a unit in SC2 is far from a win condition. Complex units must be utilized with precision and within the right contexts to reap their full value. On the other hand, worker units reflected in the predictor **WorkerUnitsMade** are units a  player may produce and subsequently assign them to indefinite valued added work. If not interrupted by an attacking force, worker units will continue to add value to the in-game economy without additional intervention from the player as long as the resource node is still abundant.

Furthermore, using complex abilities is generally much more critical than making these complex units in masses. This also compliments the initial goal of the modeling to add flavor to APMs in a way that may reveal what actions are essential and fortunately APM and this **ComplexUnitsUsed** these there are mostly orthogonal with cor `r round(cor(sc$APM,sc$ComplexAbilitiesUsed),2)`.

```{r, echo=FALSE}
sc_lm_5<-update(sc_lm_final,.~.+ComplexUnitsMade)
kable(tidy(sc_lm_5,conf.level = .05)[9:10,],caption = "ANOVA $lm_\\omega$ and $lm_\\omega+ComplexUnitsMade$")
kable(anova(sc_lm_final,sc_lm_5),digits=2)
```

#### AssigntToHotkeys and Actions Per Minute
```{r,echo=FALSE}
sc_lm_6<-update(sc_lm_final,.~.-APM)
sc_lm_7<-update(sc_lm_final,.~.-AssignToHotkeys)
```

**AssignToHotkeys and APM** have the highest correlation within the final model. **AssignToHotkeys** was considered to be dropped along with the PAC related predictors prior. Still, I imagined the predictor added a significant flavor to what type of actions regardless of its potential for confounding.**AssignToHotkeys** in-game is when a player assigns units or buildings to hotkeys. For example, if the player has two armies within a match, they may select all of the units in army one and use the hotkey combination *CTRL+1* to assign those units to hotkey *1* for future rapid selection. Then the same player can select their second army use *Ctrl+2* to hotkey *2*. This works for any commandable unit or building in-game, allowing the player to reshape hotkeys based as assets are gained or lost. 

Both variables will be removed from the model one at a time and then compared to the final model that contains both. In both subset models' **RSS** are statistically significant different. In terms of the impact to $adjR^2$, as **APM** seems to a explain a significant amount more than **AssignToHotkeys** as the models have with $adjR^2$ of `r round(summary(sc_lm_6)[["adj.r.squared"]],2)` and `r round(summary(sc_lm_7)[["adj.r.squared"]],2)` compared to the final model `r round(summary(sc_lm_final)[["adj.r.squared"]],2)`. To reaffirm the initial effort of dropping predictors that confound heavily with **APM** , **AssignToHotkeys** will be dropped from future modeling.

```{r,echo=FALSE}
kable(anova(sc_lm_final,sc_lm_6),digits=3,caption = "ANOVA $lm_\\omega$ and $lm_\\omega-APM$")
kable(anova(sc_lm_final,sc_lm_7),digits=3,caption = "ANOVA $lm_\\omega$ and $lm_\\omega-AssignToHotkeys$")
```

#### Hours Metrics
```{r,echo=FALSE}
sc_lm_8<-update(sc_lm_final,.~.-TotalHours)
sc_lm_9<-update(sc_lm_final,.~.-HoursPerWeek)
```
Upon closer examination of the remaining predictors its surprising that **TotalHours** and **HoursPerWeek** do not have a higher correlation. I did not expect both two make it to the final model. This may have to do with the reporting method. Regardless of the low correlation it is unclear if not impossible how to vary these two factors independently.

Both variables will be removed from the model one at a time and then compared to the final model that contains both. In both subset models' **RSS** are statistically significant different. In terms of the impact to $adjR^2$, as **TotalHours** seem to a explain a significant amount more than **HoursPerWeek** as the models have with $adjR^2$ of `r round(summary(sc_lm_8)[["adj.r.squared"]],2)` and `r round(summary(sc_lm_9)[["adj.r.squared"]],2)` compared to the final model `r round(summary(sc_lm_final)[["adj.r.squared"]],2)`. **HoursPerWeek** will be dropped for the final model.
```{r,echo=FALSE}
kable(anova(sc_lm_final,sc_lm_8),digits=3,caption = "ANOVA $lm_\\omega$ and $lm_\\omega-TotalHours$")
kable(anova(sc_lm_final,sc_lm_9),digits=3,caption = "ANOVA $lm_\\omega$ and $lm_\\omega-HoursPerWeek$")
```

#### Revised $lm_\omega$

After performing the changes for confounding variables, the final model results in the following coefficients. This model will be rummaged through more expensively with better tools for the final portion of this assignment.

```{r,echo=FALSE,collapse=TRUE}
sc_lm_final<-update(sc_lm_final,.~.-AssignToHotkeys-HoursPerWeek)
kable(tidy(sc_lm_final,conf.level = .05),caption = "$lm_\\omega$-HoursPerWeek-AssignToHotkeys")
kable(confint(sc_lm_final))
```
## Structural Uncertainty and Predictions

Without using cross-validation, we can see some fundamental issues with the fitted values of each model reviewed. The predictions of the model behavior and accuracy comparable between $\Omega$ & $\omega$, so only $\omega$ will be used in the following section.

The first plot is a scatterplot with the fitted values by dominant predictor APM. Some of the players are excepted to have a LeagueIndex > 10, which does not exist in sampled response. Although LeagueIndex > 7 does tease that the Grandmaster tier is an unbounded tier in terms of skill points. Overall, the continuous fitted values fail to capture the discrete nature of the response. 

The fitted values will be flattened such that when LeagueIndex > 7 will be set LeagueIndex=7, then each value will be rounded, so only integer values remain to help interpret the results of the model.

```{r, compact=TRUE,collapse=TRUE,echo=FALSE}

sc_lm_rounded<-sc_lm_final
sc_lm_rounded$fitted.values[sc_lm_rounded$fitted.values>7]<-7
sc_lm_rounded$fitted.values<-round(sc_lm_rounded$fitted.values)

p1<-ggplot(sc_lm_final, aes(x=APM, y=factor(LeagueIndex)))+geom_point()+geom_point(aes(y=fitted(sc_lm_final),color='Fitted'))+ggtitle("Fitted Values by \nAPM")+theme_classic()

p2<-ggplot(sc_lm_final, aes(x=APM, y=factor(LeagueIndex)))+geom_jitter() +geom_jitter(aes(y=round(sc_lm_rounded$fitted.values),color='Fitted'))+ggtitle("Fitted Values Flattned/Rounded \n by APM , with jitter.")+theme_classic()

grid.arrange(p1,p2,ncol=2)
```

To assess the predictive accuracy of the model the flattened and rounded values fitted values are then compared to the sampled values overall providing a `r round(sum((sc$LeagueIndex==round(sc_lm_rounded$fitted.values)))/count(sc),2)*100`% accuracy. This is a very low accuracy considering 100% of the data was used to train the model. The correct % by **LeagueIndex** is plotted below accompanied by a base rate % that reflects the chance of guessing the correct **LeagueIndex** using: 
$$BaseRate\%=n_{LeagueIndex=i}/n_{total}$$. 

```{r,echo=FALSE,error=FALSE,warning=FALSE}
sc_predicted<-data.frame("LeagueIndex"=sc$LeagueIndex,"FlattenFitted"=sc_lm_rounded$fitted.values)

sc_predicted$correct<-sc_predicted$LeagueIndex==sc_predicted$FlattenFitted

sc_predicted_agg<-sc_predicted%>%group_by(LeagueIndex)%>%
  add_tally()%>%summarise(correct_perc=sum(correct)/max(n),incorrect_perc=1-sum(correct)/max(n),n=max(n))%>%mutate(baserate=n/3317)%>%select(LeagueIndex,correct_perc,baserate)%>%
  melt(id="LeagueIndex")

ggplot(sc_predicted_agg,aes(x=LeagueIndex,y=value,fill=variable))+
   geom_bar(stat="identity", width=.5, position = "dodge")+theme_classic()+ylab("Correct %")+ggtitle("Pseudo Confusion Matrix")
```

Another part of the models bias is that it does not place anyone below **LeagueIndex=** `r min(round(fitted(sc_lm)))`. The behavior of the ordinal response severely limits the predictive power of this model. The model has infinite more likelihood of predicting someone as **LeagueIndex** = 4 who is actually **LeagueIndex**=4  then predicting someone in **LeagueIndex**=1 who is actually **LeagueIndex**=1. Less severe lopsidedness can be seen through the varying **LeagueIndexs' Correct %**. Specifically, **Correct %** for **LeagueIndex** 3 to 7 perform much better than the base rate. These sign shows are the predictive power is heavily biased towards higher **LeagueIndex**

## Conclusion

### Research's Explanatory Power

The initial goal was to add some flavor to **APM** concerning what makes players highly ranked. Due to structure issues, this model overall performs very poorly. As a result, it needs to be kept in mind that the following predictors explain less than half the variation observed in the response.

From this analysis, we can see that the amount in terms of **TotalHours** a player dedicates seems to a significant predictor, which is a good sign for players willing to invest time into their craft. The suggested rate of progress is a bit disheartening as we expected the mean response of **LeagueIndex** to increase by 1 per 5000 **TotalHours**. 

Using the predictor **WorkersMade**, one worker every 2.65 seconds is expected to increase mean LeagueIndex by one. Workers drive a player's economy, and the professionals always seems to have them in enormous quantities.

One minimap attack per 12.6 seconds is expected to increase mean **LeagueIndex** by one. Minimap attacks save the player from changing their main view for each attack command by allowing them to command a unit to any point using the minimap. For example, suppose we used the hotkey assignments mentioned previously. In that case, a player could press *1* and attack-click somewhere on the minimap to command an entire army to attack a location without changing the player's field of view. In a match where there are skirmishes across the map, a player more skilled at minimap attacks would be expected to have a strong advantage in managing multiple battlefronts near simultaneously. This may be a good target skill for players trying to increase their **LeagueIndex**.

One complex ability used per 3.3 seconds is expected to increase mean **LeagueIndex** by one. Even without the confounding variable in this model, it is still difficult to translate this idea into in-game practice because these Complex units need to be made before their abilities can be used. To construct these used, a player needs to have a relatively strong economy that may not be obtainable until midmatch. 

It would take 1250 unique hotkeys used per second to increase mean **LeagueIndex** by one. This variable's units are  incorrect or the effect is feeble. No additional information was provided by the data source to further troubleshoot @UCI.

Finally, **APM** increases **LeagueIndex** by one per 68. It's not surprising that speed seems to have a dominating effect.

### Prelude to final

For the final, the logistical regression will remodel the same problem with a different set of techniques and assumptions that fit the ordinal response. The second iteration is likely to be much smoother because of this exploratory analysis's heavy leg work.

 -- End Midterm 2 -- 

# Introduction To Final

```{r loadingmass,include=FALSE}
#Final
library(car) # VIF ,Final
library(caret) #for easy machine learning workflow
library(leaps) #for computing stepwise regression
#require(foreign)
library(ordinal)
```

The final research question aligns with the initial research question's scope, which is to explain what makes players highly ranked. The last effort was limited because it impromptu reduced the span of the model using p-values, only dabbled in diagnostics. More importantly, it applied a multivariant linear regression to an ordinal response. 

To select better combination of predictors, the final analysis once again utilizes MLR but instead in combination with stepwise regression. The stepwise regression $lm_\circlearrowleft$ provides a more exhaustive examination of the combination of predictors than the initial $lm_\omega$. With $lm_\omega$ many predictors were trimmed prior to modeling based on the concern that they may be colinear APMs thus hindering the model's explaintory power. These predictors will be reinstated for $lm_\circlearrowleft$'s selection because the Bayesian information criterion (BIC) should account for the variance inflation and trim accordingly. Should any of these predictors survive the trimming, they will be re-examined. 

Subsequently, additional diagnostics will be performed iteratively with remediation as needed ^[Four iterations of preliminary diagnostics/remediation on $lm_\omega$ revealed **TotalHours** contained four points with high leverage with Cook's Distance greater than 1. To prevent this analysis from ballooning due to excessive remediation, a log transform on **Totalhours** will be performed before creating $lm_\circlearrowleft$. At the same time, $lm_\omega$ will be left the same to provide a stark constant between the initial naive approach and more formal model selection. **TotalHours** does not contain any 0 values.].

```{r,echo=FALSE}
sc_n<-sc_n%>%mutate(TotalHours=log(TotalHours))
```

The last model $ord_\circlearrowleft$ will use ordinal regression with the same predictors as $lm_\circlearrowleft$ to provide a model fitted to the ordinal nature of the response while keeping selection of predictors streamline ^[The same set of predictors will be used mainly because I lack a package to do stepwise ordinal regression]. Finally $lm_\circlearrowleft$, $lm_\omega$, and $ord_\circlearrowleft$  will be compared using BIC and predictive power.


# Stepwise Regression

```{r, echo=FALSE}
sc_sw<-regsubsets(LeagueIndex~.-GameID,sc_n , nvmax = 25,
                     method = "exhaust")
sc_sw_summary<-summary(sc_sw)
```


Regsubsets will be used to generate the stepwise model ^[The function summary() reports the best set of variables for each model size. From the output above, an asterisk #specifies that a given variable is included in the corresponding model @leaps .]. 

Diving into the predictors provided by the stepwise regression, its clear there is a bit of a shake up concerning which predictors are selected. **ActionLatency**  instead of **APMs** seems to dominate the model selection. These two predictors need to be further explored to understand their colinearity. Compared to $lm_\omega$, **ActionLatency** made the cut after being forced out in the initial model select. **TotalHours, AssignToHotkeys, APM, and MinimapAttacks** are all common between the two models. **ComplexAbilitiesUsed** was contained in $lm_\omega$ but did not survive the 5 predictor model trimming. It is not until including 18 out of 19 predictors does  **ComplexAbilitiesUsed** make it into the model.

```{r, echo=FALSE}
plot(sc_sw_summary[['bic']]-min(sc_sw_summary[['bic']]),ylab="BIC",xlab="Index(1=One Predictor Model)",main ="Stepwise Modeling BIC")

kable(sc_sw_summary[["outmat"]][1:5,c("ActionLatency","TotalHours",
    "AssignToHotkeys","APM","MinimapAttacks")])
```

Choosing the best model provided by five predictors intuitively seemed like a good route for explanatory purposes as a "Top 5 Skills for aspiring starcraft players" sounds like a good starting point, but diminishing returns concerning BIC also seems to support predictors 5 as a reasonable cutoff.

# Diagonostics

Knowing that $lm_\omega$ & $lm_\circlearrowleft$  has been uncharacteristically fitted to the ordinal response most of the diagnostics and related assumptions will be consistently violated regardless of transformations within MLR modeling.

## Confounding

It was was a initial concern that **ActionLatency** and **TotalHours** would be collinear. Plotting the correlation matrix of the predictor span of $lm_\circlearrowleft$:

```{r, echo=FALSE}
sc_cor<-cor(sc_n[,c("ActionLatency","APM","TotalHours","MinimapAttacks","AssignToHotkeys")],use = "complete.obs")
sc_cor_plot<-corrplot(sc_cor,
    tl.cex=.75,
    tl.col='black',
    type="lower",diag=FALSE, method="number")
```

To further explore the impact of these two variables. The `update()` function is used to remove each **APM & ActionLatency** variable one at a time to compare the effects of dropping them respectively to the original model. BIC, Max VIF, and Adjusted $R^2$ will be used to decide the which model will be best for explanatory purposes ^[An ANOVA test was performed on between each subset model and the original $lm_\circlearrowleft$.Both had significant p-values showing they have significantly different residuals, these results were not shown because of the nongaussianity of the residuals of both models] .

```{r, echo=FALSE}

sc_sw_lm<-lm(LeagueIndex~ActionLatency+TotalHours+AssignToHotkeys+APM+MinimapAttacks,sc_n)

kable(vif(sc_sw_lm),
    col.names = "VIF",
    digits = 3,
    caption = paste("$lm_\\circlearrowleft$ VIF Table. $R^2=$",
      round(summary(sc_sw_lm)[["adj.r.squared"]],2),
      " $BIC=$",round(BIC(sc_sw_lm),0)
      )
      )

kable(vif(update(sc_sw_lm,.~.-APM)),
    col.names = "VIF",
    digits = 3,
    caption = paste("$lm_\\circlearrowleft-APM$ VIF Table. $R^2=$",
      round(summary(update(sc_sw_lm,.~.-APM))[["adj.r.squared"]],2),
      " $BIC=$",round(BIC(update(sc_sw_lm,.~.-APM)),0)
      )
      )

kable(vif(update(sc_sw_lm,.~.-ActionLatency)),
    col.names = "VIF",
    digits = 3,
    caption = paste("$lm_\\circlearrowleft-ActionLatency$ VIF Table. $R^2=$",
      round(summary(update(sc_sw_lm,.~.-ActionLatency))[["adj.r.squared"]],2),
      " $BIC=$",round(BIC(update(sc_sw_lm,.~.-ActionLatency)),0)
      )
      )
```

This comparison makes it clear that the effects are dropping APM are less severe on the models overall predictive power also are more effective for reducing the VIF. **APM** is has been a clear front runner of predict power until this point so some additional diagnostics will be performed before judging that it is fair to remove **APM**. The additional diagnostic will take the form of a linear model `lm(ActionLatency~.)` to pry out additional insight about the colinearity using VIF. The VIF of the two models `lm(ActionLatency~.)` and `lm(ActionLatency~.-APM)` are plotted to show what the inclusion of **APM** does.

```{r , echo=FALSE}
lm_al<-lm(ActionLatency~.-GameID,sc_n)
lm_al_vif<-data.frame(rbind(cbind(round(vif(lm_al),1),"All Pred."),
    cbind(round(vif(update(lm_al,.~.-APM)),1),"W\\O APM")))

colnames(lm_al_vif)<-c("VIF","Model")

ggplot(lm_al_vif,aes(x=reorder(row.names(lm_al_vif),-1*as.numeric(VIF)),y=as.numeric(VIF),fill=Model))+
   geom_bar(stat="identity", width=.5, position = "dodge")+theme_classic()+ylab("VIF")+xlab("Predictor")+ggtitle("VIF When Predicting ActionLatency")+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

The effects of APM clearly muddle the the usability of other predictors. **APM** provided a starting point for what makes Starcraft II players highly ranked, but in attempts to expand the description of what makes players good **APM** clearly stands in the way of the possibly more subtle predictors. The model will be remediated to force out APMs.

## Stepwise Remediation

Reiterating the stepwise model selection process used previously while forcing out **APM**, the five predictor variable returned this time were **ActionLatency, TotalHours, AssigntoHotkeys, SelectByHotkeys, and MinimapAttacks**. Where **SelectByHotkeys** took the the place of **APM**. The BIC of this model fall slightly above that that included both **APM & ActionLatency** but below the models that dropped either. Subsequently the same steps were performed for colinearity and the remediation of forcing out **APM** was effective for reducing the overall potential for confoundance.

```{r,echo=FALSE}
sc_sw<-regsubsets(LeagueIndex~.-GameID,sc_n , nvmax = 25,
                     method = "exhaust",force.out = "APM")
sc_sw_summary<-summary(sc_sw)

sc_sw_lm<-lm(LeagueIndex~ActionLatency+TotalHours+AssignToHotkeys+SelectByHotkeys+MinimapAttacks,sc_n)
kable(vif(sc_sw_lm),
    col.names = "VIF",
    digits = 3,
    caption = paste("$lm_\\circlearrowleft$ with remediation VIF Table. $R^2=$",
      round(summary(sc_sw_lm)[["adj.r.squared"]],2),
      " $BIC=$",round(BIC(sc_sw_lm)),0)
      )

sc_cor<-cor(sc_n[,c("ActionLatency","SelectByHotkeys","TotalHours","MinimapAttacks","AssignToHotkeys")],use = "complete.obs")
sc_cor_plot<-corrplot(sc_cor,
    tl.cex=.75,
    tl.col='black',
    type="lower",diag=FALSE, method="number")
```


## Gauss Markov:

In midterm 2 Shapiro-Wilks tests for normality were performed on the ordinal response **LeagueIndex** and the residuals of $lm_\omega$, both were found have have nongaussian residuals. The same holds true for $lm_\circlearrowleft$. This prevents the our regression models from being the maximum likelihood estimators and prevents the application of ANOVA. The following plots show the distribution of actual LeagueIndexs and the fitted values from our two models. Density plots were chosen instead of histograms in this situation because although the actual **LeagueIndex** are ordinal the fitted values are continuous so this helps with the comparability.

```{r, echo=FALSE}
lm_freqs<-data.frame(rbind(
cbind(fitted.values(sc_lm_final),"omega"),
cbind(fitted.values(sc_sw_lm),"sw"),
cbind(sc_n$LeagueIndex,"act.")
))

colnames(lm_freqs)<-c("LeagueIndex","Model")

ggplot(lm_freqs, aes(x=as.numeric(LeagueIndex), fill=Model)) + geom_density(alpha=0.2)+xlab("LeagueIndex")
```
The generation of an ordinal model is the most appropriate way to proceed, but to exercise the requirements for final report the remainder of the diagnostics try to address other problems concerning linear regression model assumptions.

## Residuals Vs Fitted and Standardized:

The Residuals vs Predictors/fitted plots reveals some odd behavior around the tails. Concerning the Residuals Vs Fitted Values, on the left there are sparse amount of fitted values with some being outside the possible range of **LeagueIndex**. Overall the left end of the tail is biased to over estimate. On the right there are also a sparse amount of fitted values with some greater then the possibilities for **LeagueIndex** with some bias to overestimate

$lm_\omega$ fitted values have a very long right tail that places players with a **LeagueIndex** greater than Grandmaster (**LeagueIndex**=seven) and a short left tail that does not place any player lower silver (**LeagueIndex**=2). At both tails the model is bias to greatly overestimate while center quartiles tend to more subtly underestimate. Upon closer examination this bias could be driven by **TotalHours, APM, MiniMapAttacks, WorkersMade, and ComplexUnitsMade**. It might be reasonable to project that this model would suffer from nonnormal residuals even if the response was continuous.

```{r, echo=FALSE}
residualPlots(sc_lm_final,tests=FALSE)
```

On the other hand $lm_\circlearrowleft$ visually performs much better than $lm_\omega$ as the fitted vs residual smoothed line seems to have intercept and slope approximately 0. The log transformation performed on **TotalHours** appears to have mitigated the trend between residuals and the predictor's values. **ActionLatency, AssigntoHotkeys, APM, and MinimapAttacks** have trends with their respective values and the residuals. Transformation on these predictors were considered but not taken because it would complicate their explanatory power and the transforms would do little to mitigate the issues of applying a linear model to an ordinal value.

```{r, echo=FALSE}
residualPlots(sc_sw_lm,tests=FALSE)
```

## Cook's Distance

When plotting the cook's distance for $lm_\omega$ initially three points have data points with large cook's distance. As previously mentioned in the footnotes some preemptive diagnostics were performed on $lm_\omega$ and the high leverage outliars discovered stem from **TotalHours**, thus a log transform was performed prior to creating $lm_\circlearrowleft$ to sidestep this issue. Based on $lm_\circlearrowleft$ Cook's Distance plot no additional remediation will be made for this model.

```{r, echo=FALSE}
plot(sc_lm_final,which = 4)
```

## BoxCox

Transformations of the response would hamper the explanatory power of this analysis, but as an exercise the following uses Box-Cox Plot (for $lm_\circlearrowleft$) to explore if any transformations of the response would be useful. From the following we can see the lambda value fall slightly above 1 thus no transformation of the response is encouraged. This is persistent between both models.

```{r, echo=FALSE}
boxCox(sc_sw_lm)
```

## Remaining Diagnostics

The remaining diagnostics do not reveal anything more clearly then the previously chosen diagnostics.

# Portional Odds model

To construct a ordinal regression model using the the `ordinal` package and function `clm` using the same parameters as final model $lm_\circlearrowleft$ @ordinal :

$$log(\frac{p}{1-p})=\alpha+\beta_1x_1+\beta_2x_2...+\beta_px_p\\ vs. \\log(\frac{p_i}{1-p_i})=\alpha_i+\beta_1x_1+\beta_2x_2...+\beta_px_p$$

Logistic regression predicts $p$,the probability of a binomial response, alternatively ordinal regression predicts $p_i$ the predictors coefficient partitioned by  response $i \in LeagueIndex$. This encoding predicts the logit odds based on a set of parameters $X$. For ordinal regression to be applied appropriately the parallel slopes assumption needs to hold. Validating that it does is far outside the scope of this course, so subsequent diagnostics will not be performed on this model. 

Using the predict function returns the likelihood estimates for bank for each player, we can use the `type=class` so the turn to mimic a fitted value for each of data points. We can see that the fitted values of this model are much more evenly distributed across each of the 7 league indexes while having a favor for **LeagueIndex** 1. Further more we can see that the BIC is lower then both of the previous models.

```{r, echo=FALSE,warning=FALSE}
m <- clm(factor(LeagueIndex) ~ ActionLatency+TotalHours+AssignToHotkeys+SelectByHotkeys+MinimapAttacks, data = sc_n, na.action = na.omit)
hist(as.numeric(unlist(predict(m,sc_n,type="class"))),main = paste("Fitted Values of Proportional Odds Model BIC=",round(BIC(m))),nclass = 8,xlab = "LeagueIndex")

m1 <- clm(factor(LeagueIndex) ~ ActionLatency, data = sc_n, na.action = na.omit)
m2 <- clm(factor(LeagueIndex) ~ ActionLatency+TotalHours, data = sc_n, na.action = na.omit)
m3 <- clm(factor(LeagueIndex) ~ ActionLatency+TotalHours+AssignToHotkeys, data = sc_n, na.action = na.omit)
m4 <- clm(factor(LeagueIndex) ~ ActionLatency+TotalHours+AssignToHotkeys+SelectByHotkeys, data = sc_n, na.action = na.omit)
m5 <- clm(factor(LeagueIndex) ~ ActionLatency+TotalHours+AssignToHotkeys+SelectByHotkeys+MinimapAttacks, data = sc_n, na.action = na.omit)

sw1 <- lm(LeagueIndex ~ ActionLatency, data = sc_n, na.action = na.omit)
sw2 <- lm(LeagueIndex  ~ ActionLatency+TotalHours, data = sc_n, na.action = na.omit)
sw3 <- lm(LeagueIndex  ~ ActionLatency+TotalHours+AssignToHotkeys, data = sc_n, na.action = na.omit)
sw4 <- lm(LeagueIndex  ~ ActionLatency+TotalHours+AssignToHotkeys+SelectByHotkeys, data = sc_n, na.action = na.omit)
sw5 <- lm(LeagueIndex  ~ ActionLatency+TotalHours+AssignToHotkeys+SelectByHotkeys+MinimapAttacks, data = sc_n, na.action = na.omit)
```

# Comparison between models

The following uses BIC to compare the iteratively selected models based on trimming the least significant p-value seen in midterm 2 , the exhaustive stepwise regression for all possible size combinations, and the proportional Odds model parallel the predictor selection from stepwise. The starting point for the $lm_\omega$ is all predictors while the latter two $lm_\circlearrowleft$ & $ord_\circlearrowleft$ start with one predictor and increase in model size over subsequent iterations. Overlap between the two MLR models concerning BIC is not expected because the first series, based on pvalues, had the many potentially colinear variables trimmed before any modeling was conducted while the stepwise regression starts with one predictor and finishes with the inclusion of all predictors. Another difference is the log transform on **TotalHours**.

```{r bicplot, echo=FALSE}
pval_bic<-cbind(c(BIC(sc_lm),BIC(sc_lm_1),BIC(sc_lm_2),BIC(sc_lm_3),
    BIC(sc_lm_4),BIC(sc_lm_final)),"PValue")%>%data.frame()
pval_bic$ModelSequence <- 1:nrow(pval_bic)

sw_bic<-cbind(c(BIC(sw1),BIC(sw2),BIC(sw3),BIC(sw4),BIC(sw5)),"SW")%>%data.frame()
sw_bic$ModelSequence<- 1:nrow(sw_bic)

ord_bic<-cbind(c(BIC(m1),BIC(m2),BIC(m3),BIC(m4),BIC(m5)),"Ord")%>%data.frame()
ord_bic$ModelSequence<- 1:nrow(ord_bic)

lm_bic<-rbind(sw_bic,pval_bic,ord_bic)
lm_bic$X1<-as.numeric(as.character(lm_bic$X1))
lm_bic$X1<-lm_bic$X1-min(lm_bic$X1)
ggplot(lm_bic,aes(ModelSequence,y=X1,color=X2))+ geom_point()+
  geom_smooth(method=lm, se=FALSE)+ylab("BIC-Min(BIC)")+theme_classic()
```

From this plot we can see that the ordinal models provide the min BIC in all comparable instances. This was expected because the ordinal model is fitted to the ordinal response. 

# Goodness of fit

Using the same methods to round and flatten as seen in midterm 2 $lm_\circlearrowleft$ was used to produce predictions. A notable improvement over $lm_\omega$ is that this model actual predicts players with **LeagueIndex** less than 2. The overall model accuracy increased by 5% from 34% to 39%.

```{r, compact=TRUE,collapse=TRUE,echo=FALSE}
sc_sw_lm_rounded<-sc_sw_lm
sc_sw_lm_rounded$fitted.values[sc_sw_lm_rounded$fitted.values>7]<-7
sc_sw_lm_rounded$fitted.values<-round(sc_sw_lm_rounded$fitted.values)


sc_sw_predicted<-data.frame("LeagueIndex"=sc_n$LeagueIndex,"FlattenFitted"=sc_sw_lm_rounded$fitted.values)

sc_sw_predicted$correct<-sc_sw_predicted$LeagueIndex==sc_sw_predicted$FlattenFitted

sc_sw_predicted_agg<-sc_sw_predicted%>%group_by(LeagueIndex)%>%
  add_tally()%>%summarise(correct_perc=sum(correct)/max(n),incorrect_perc=1-sum(correct)/max(n),n=max(n))%>%mutate(baserate=n/3317)%>%select(LeagueIndex,correct_perc,baserate)%>%
  melt(id="LeagueIndex")

ggplot(sc_sw_predicted_agg,aes(x=LeagueIndex,y=value,fill=variable))+
   geom_bar(stat="identity", width=.5, position = "dodge")+theme_classic()+ylab("Correct %")+ggtitle(paste("Stepwise Regression Pseudo Confusion Matrix, Acc",round(sum((sc_n$LeagueIndex==round(sc_sw_lm_rounded$fitted.values)))/count(sc),2)*100,"%"))
```
On the other hand $ord_\circlearrowleft$ performs dramatically worse with an accuracy of 15%. This may have to do with my preparation of the model or use of `predict(m,sc_n,type="class")` to obtain the fitted values.
```{r, echo=FALSE}
sc_ord_predicted<-data.frame("LeagueIndex"=sc_n$LeagueIndex,"FlattenFitted"=as.numeric(unlist(predict(m,sc_n,type="class"))))
sc_ord_predicted$correct<-sc_ord_predicted$LeagueIndex==sc_ord_predicted$FlattenFitted
sc_ord_predicted_agg<-sc_ord_predicted%>%group_by(LeagueIndex)%>%
  add_tally()%>%summarise(correct_perc=sum(correct)/max(n),incorrect_perc=1-sum(correct)/max(n),n=max(n))%>%mutate(baserate=n/3317)%>%select(LeagueIndex,correct_perc,baserate)%>%
  melt(id="LeagueIndex")

ggplot(sc_ord_predicted_agg,aes(x=LeagueIndex,y=value,fill=variable))+
   geom_bar(stat="identity", width=.5, position = "dodge")+theme_classic()+ylab("Correct %")+ggtitle(paste("Ordinal Regression Pseudo Confusion Matrix, Acc",round(sum((sc_n$LeagueIndex==sc_ord_predicted$FlattenFitted))/count(sc),2)*100,"%"))

```

# Explanatory Power

One model is the obvious choice to be used for explanatory purposes and that is $lm_\circlearrowleft$. $lm_\omega$ is limited BIC because the lack of rigor in terms of selecting predictors and $ord_\circlearrowleft$ has some major issues I am not equipped to address. Between the last explanation of $lm_\omega$ and $lm_\circlearrowleft$ only some of the predictors have switched.

**APM** of the predictors removed from the model this was by far the most significant. **APM** colinearity with the other variables was too great to be included in the model. This remediation provides the model with a lot more certainty because the remaining predictors have much less covariance reflected in the measure of BIC. **ActionLatency** seems to have usurped **APM** because it manages to capture quickness while remaining much more orthogonal to the other predictors. **ActionLatency** describes the average delay after moving the players screen before taking the first action. A player with high **ActionLatency** would move their screen to a new focus and rapidly take action. This does not necessarily imply they are constantly moving their screen. I think this is a good reflection of the magnitude of a players intent. Players who are unsure how to dynamic react scenario's may switch frames, pause and digest the situation, and then react accordingly. Players who are confident in their play while minimize this time by knowing what to expect from the opponent or because of their ability to understand battle states from their minimap prior to switching their focus and taking action. This dicussion also highlights another persistent predictor and that is **MiniMapAttacks**, thorougly dicussed in midterm 2, **MiniMapAttacks** reflects the amount of attacks the player takes on their minimap without the need of switching their main focus. Both high **MiniMapAttacks** and low **ActionLatency** may be sign of players becoming less dependent on visually digesting the battlestates before reacting. 

**UniqueHotKeys** also did not make it into the final model, **AssignToHotkeys** persisted between the two models, and **SelectByHotkey** was a new addition. This is quite a interesting change. A head to head comparison between these switches is not completely appropriate without additional testing, but this makes some good sense when viewed within the context of a match. **UniqueHotKeys** based on its definition reflects the overall unique usage of the keyboard combinations per unit of time while the other. **AssignToHotkeys** considers the rate as which the player assigns unit or building groupings. A player for example may assign unit A and unit B to hotkey 1, and subsequently overwrite that assignment with unit C later. From the  **UniqueHotKeys** perspective it would look like this player only using one type of action per unit of time instead of reflecting the underline dynamics of the selection change. For this reason it does seem like better encouragement to tell a player to increase their **AssignToHotkeys** instead of **UniqueHotKeys**. **SelectByHotkey** when used in tandem and may reflect **AssignToHotkeys** assignments actual usage.

**WorkersMade and ComplexAbilitiesUsed** also did not make it into $lm_\circlearrowleft$ when using 5 predictors. None of the variables that made it into the final model seem like a direct replacement, so my review in midterm 2 still seems appreciate they are just likely less powerful in terms of explanation when compared to the newly introduced predictors.

# Conclusion



# Appendix

### About Columns

**Attribute Information @UCI :**

1. GameID: Unique ID number for each game (integer)
2. LeagueIndex: Bronze, Silver, Gold, Platinum, Diamond, Master, GrandMaster, and Professional leagues coded 1-8 (Ordinal)
3. Age: Age of each player (integer)
4. HoursPerWeek: Reported hours spent playing per week (integer)
5. TotalHours: Reported total hours spent playing (integer)
6. APM: Action per minute (continuous)
7. SelectByHotkeys: Number of unit or building selections made using hotkeys per timestamp (continuous)
8. AssignToHotkeys: Number of units or buildings assigned to hotkeys per timestamp (continuous)
9. UniqueHotkeys: Number of unique hotkeys used per timestamp (continuous)
10. MinimapAttacks: Number of attack actions on minimap per timestamp (continuous)
11. MinimapRightClicks: number of right-clicks on minimap per timestamp (continuous)
12. NumberOfPACs: Number of PACs per timestamp (continuous)
13. GapBetweenPACs: Mean duration in milliseconds between PACs (continuous)
14. ActionLatency: Mean latency from the onset of a PACs to their first action in milliseconds (continuous)
15. ActionsInPAC: Mean number of actions within each PAC (continuous)
16. TotalMapExplored: The number of 24x24 game coordinate grids viewed by the player per timestamp (continuous)
17. WorkersMade: Number of SCVs, drones, and probes trained per timestamp (continuous)
18. UniqueUnitsMade: Unique unites made per timestamp (continuous)
19. ComplexUnitsMade: Number of ghosts, infestors, and high templars trained per timestamp (continuous)
20. ComplexAbilitiesUsed: Abilities requiring specific targeting instructions used per timestamp (continuous)

### Why Violin Plots.

I decided to use Violin plots because I found with less tweaking they provided almost all the information I was looking for compared to scatter plots. Head to head a limitation of violin plots is that they make it seems as though the LeagueIndex level size contains the same $n$. The histogram earlier in this analysis shows clearly that $n$ at each level of the **LeagueIndex** is not equal so choosing a tool on the basis of reiterating that point seems redundant. The benefit of Violin plots is that they provide a smoothed density plot at each **LeagueIndex** with a single point that represents the mean. This same thing could be done with scatter plots but I found it took much more staring and plot to plot variation.

The following is some head to head varieties plotting the data.

```{r, echo=FALSE}
p1<-ggplot(sc,aes(x=APM,y=LeagueIndex))+ geom_point( alpha = 0.1, size = 3)+scale_fill_manual(values=cbPalette)
p2<-ggplot(sc,aes(x=APM,y=LeagueIndex, fill=factor(LeagueIndex)))+ geom_jitter(shape=21, alpha = 0.1, size = 3)+scale_fill_manual(values=cbPalette)
p3<-ggplot(sc,aes(x=APM,y=LeagueIndex))+ geom_jitter(alpha = 0.1, size = 3)
p4<-VioLeagueIndex("APM")
```

# Code

```{r code,eval=FALSE}
library(tidyverse)
library(dbplyr) #piping
library(ggplot2) #plotting
library(gridExtra)# easy plot grids
library(Hmisc) # for correlation matrix
library(corrplot) # For correlation matrix graphic
library(broom) #tidy lm summaries
library(knitr) #pretty tables
library(reshape2) #melt function

sc<-read_csv("~/STAT757/skillcraft/SkillCraft1_Dataset.csv")

colnames(sc)

cbPalette <- c("#CC6600", "#999999", "#FFCC00", "#CCCFFF", "#CCFFFF","#0072B2", "#FF6600")

#set type
sc$HoursPerWeek<-as.numeric(sc$HoursPerWeek)
sc$TotalHours<-as.numeric(sc$TotalHours)

count_missing_age<-count(sc%>%
  filter(is.na(Age))%>%arrange(LeagueIndex))
count_professional<-count(sc%>%filter(LeagueIndex==8))
count_grandmaster<-count(sc%>%filter(LeagueIndex==8))
print(paste('There are ',count_missing_age,' missing values in the age column. There are ',count_professional,' professional #players.'))

sc<-filter(sc,sc$TotalHours<1000000)

sc<-sc%>%
  drop_na()%>%
  filter(HoursPerWeek!=0)
sc_describe<-describe(sc)

sc<-sc%>%
  mutate(NumberOfPACs=NumberOfPACs*88.5,
         MinimapAttacks=MinimapAttacks*88.5,
         MinimapRightClicks=MinimapRightClicks*88.5,
         SelectByHotkeys=SelectByHotkeys*88.5,
         AssignToHotkeys=AssignToHotkeys*88.5,
         UniqueHotkeys=UniqueHotkeys*88.5,
         WorkersMade=WorkersMade*88.5,
         UniqueUnitsMade=UniqueUnitsMade*88.5,
         ComplexUnitsMade=ComplexUnitsMade*88.5,
         ComplexAbilitiesUsed=ComplexAbilitiesUsed*88.5,
         GapBetweenPACs=GapBetweenPACs*1000,
         ActionLatency=ActionLatency*1000)


LeagueIndex_Normal<-shapiro.test(sc$LeagueIndex)
  
ggplot(sc)+
  geom_histogram(aes(x=LeagueIndex,y=(..count..)/sum(..count..),fill=LeagueIndex),
      position = "identity", binwidth = 1,fill=cbPalette) +
  ylab("Relative Frequency")+
  ggtitle('LeagueIndex Distribution',subtitle = paste(LeagueIndex_Normal[3],
      " P-Value: ",LeagueIndex_Normal[2]))+xlab("LeagueIndex 1-Bronze to 7-Grandmaster")+theme_classic()

sc_cor<-cor(select_if(sc,is.numeric),use = "complete.obs")
sc_cor_plot<-corrplot(sc_cor,
    tl.cex=.75,
    tl.col='black',
    type="lower",)

sc<-sc%>%select(!c(GameID,ActionLatency,GapBetweenPACs,NumberOfPACs,SelectByHotkeys,ActionsInPAC))

cor_hoursperweek<-paste(
  round(cor(sc%>%filter(between(LeagueIndex,1,4))%>%select(LeagueIndex),
    sc%>%filter(between(LeagueIndex,1,4))%>%select(HoursPerWeek))[1],
    2),
  "vs",
  round(cor(sc%>%filter(between(LeagueIndex,4,7))%>%select(LeagueIndex),
    sc%>%filter(between(LeagueIndex,4,7))%>%select(HoursPerWeek))[1],
    2)
)

VioLeagueIndex<-function(predictor){
  ggplot(sc, aes(x=factor(LeagueIndex), y=unlist(sc%>%select(all_of(predictor))), fill=factor(LeagueIndex))) + 
    geom_violin(trim=FALSE, color="black")+scale_fill_manual(values=cbPalette)+
    stat_summary(fun.data=mean_sdl,geom="pointrange", color="black")+ coord_flip()+
    ggtitle(paste("LeagueIndex by",predictor))+
    xlab("LeagueIndex")+ylab(predictor) + guides(fill= FALSE)+theme_classic()
    }

plots<-lapply(colnames(sc)[2:length(colnames(sc))],VioLeagueIndex)

do.call("grid.arrange", c(plots[1:4], ncol=2))
do.call("grid.arrange", c(plots[5:8], ncol=2))
do.call("grid.arrange", c(plots[9:13], ncol=2))

sc_lm<-lm(LeagueIndex~.,sc)
sc_lm_1<-update(sc_lm,.~.-UniqueUnitsMade ,sc)
sc_lm_2<-update(sc_lm,.~.-Age-UniqueUnitsMade)
sc_lm_3<-update(sc_lm,.~.-Age-UniqueUnitsMade-ComplexUnitsMade)
sc_lm_4<-update(sc_lm,.~.-Age-UniqueUnitsMade-ComplexUnitsMade-MinimapRightClicks)
sc_lm_final<-update(sc_lm,.~.-Age-TotalMapExplored-UniqueUnitsMade-MinimapRightClicks-ComplexUnitsMade)

kable(anova(sc_lm,sc_lm_final),digits=2)

#t<-data.frame(confint(sc_lm),confint(sc_lm_final))
t1 <- data.frame(confint(sc_lm))
t2 <- data.frame(confint(sc_lm_final))
t3<-merge(t1,t2,by="row.names",all=TRUE)
t3<-t3%>%rename(LL_s=X2.5...x,UL_s=X97.5...x,LL_f=X2.5...y,UL_f=X97.5...y)
t3<-t3%>%mutate(mean_s=(UL_s+LL_s)/2,
                mean_f=(UL_f+LL_f)/2,
                delta=(mean_f-mean_s)/mean_f,
                delta_width=((UL_f-LL_f)-(UL_s-LL_s))/(UL_f-LL_f))%>%
         mutate_if(is.numeric, ~round(., 3))
kable(t3,format = "markdown", digits = c(4,4,4,4,4,4,4,2,2), caption="Confidence Intervals Statistics at $\\alpha$ 0.05" )

sc_omega<-sc%>%select(HoursPerWeek,TotalHours,HoursPerWeek,APM,AssignToHotkeys,WorkersMade,ComplexUnitsMade,ComplexAbilitiesUsed)

sc_omega_cor<-cor(select_if(sc_omega,is.numeric),use = "complete.obs")
sc_omega_cor_plot<-corrplot(sc_omega_cor,
    tl.cex=.75,
    tl.col='black',
    method="number",
    type="lower")

sc_lm_5<-update(sc_lm_final,.~.+ComplexUnitsMade)
kable(tidy(sc_lm_5,conf.level = .05)[9:10,],caption = "ANOVA $lm_\\omega$ and $lm_\\omega+ComplexUnitsMade$")
kable(anova(sc_lm_final,sc_lm_5),digits=2)

sc_lm_6<-update(sc_lm_final,.~.-APM)
sc_lm_7<-update(sc_lm_final,.~.-AssignToHotkeys)

kable(anova(sc_lm_final,sc_lm_6),digits=3,caption = "ANOVA $lm_\\omega$ and $lm_\\omega-APM$")
kable(anova(sc_lm_final,sc_lm_7),digits=3,caption = "ANOVA $lm_\\omega$ and $lm_\\omega-AssignToHotkeys$")

sc_lm_8<-update(sc_lm_final,.~.-TotalHours)
sc_lm_9<-update(sc_lm_final,.~.-HoursPerWeek)

kable(anova(sc_lm_final,sc_lm_8),digits=3,caption = "ANOVA $lm_\\omega$ and $lm_\\omega-TotalHours$")
kable(anova(sc_lm_final,sc_lm_9),digits=3,caption = "ANOVA $lm_\\omega$ and $lm_\\omega-HoursPerWeek$")

sc_lm_final<-update(sc_lm_final,.~.-AssignToHotkeys-HoursPerWeek)
kable(tidy(sc_lm_final,conf.level = .05),caption = "$lm_\\omega$-HoursPerWeek-AssignToHotkeys")
kable(confint(sc_lm_final))

sc_lm_rounded<-sc_lm_final
sc_lm_rounded$fitted.values[sc_lm_rounded$fitted.values>7]<-7
sc_lm_rounded$fitted.values<-round(sc_lm_rounded$fitted.values)

p1<-ggplot(sc_lm_final, aes(x=APM, y=factor(LeagueIndex)))+geom_point()+geom_point(aes(y=fitted(sc_lm_final),color='Fitted'))+ggtitle("Fitted Values by \nAPM")+theme_classic()

p2<-ggplot(sc_lm_final, aes(x=APM, y=factor(LeagueIndex)))+geom_jitter() +geom_jitter(aes(y=round(sc_lm_rounded$fitted.values),color='Fitted'))+ggtitle("Fitted Values Flattned/Rounded \n by APM , with jitter.")+theme_classic()

grid.arrange(p1,p2,ncol=2)

sc_predicted<-data.frame("LeagueIndex"=sc$LeagueIndex,"FlattenFitted"=sc_lm_rounded$fitted.values)

sc_predicted$correct<-sc_predicted$LeagueIndex==sc_predicted$FlattenFitted

sc_predicted_agg<-sc_predicted%>%group_by(LeagueIndex)%>%
  add_tally()%>%summarise(correct_perc=sum(correct)/max(n),incorrect_perc=1-sum(correct)/max(n),n=max(n))%>%mutate(baserate=n/3317)%>%select(LeagueIndex,correct_perc,baserate)%>%
  melt(id="LeagueIndex")

ggplot(sc_predicted_agg,aes(x=LeagueIndex,y=value,fill=variable))+
   geom_bar(stat="identity", width=.5, position = "dodge")+theme_classic()+ylab("Correct %")+ggtitle("Pseudo Confusion Matrix")
```
# References